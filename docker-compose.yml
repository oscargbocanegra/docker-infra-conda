# =====================================================
# Docker Compose: Conda Development Environment
# Miniconda + Mamba + JupyterLab + GPU NVIDIA
# Actualizado: 21 de noviembre de 2025
# =====================================================

services:
  conda-jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: conda-jupyter
    hostname: conda-dev
    
    ports:
      - "192.168.80.200:8888:8888"  # JupyterLab
      - "192.168.80.200:7860:7860"  # Gradio UI
    
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
      - CHOWN_HOME=yes
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      
      # HuggingFace configuration
      - HF_HOME=/root/.cache/huggingface
      - HF_DATASETS_CACHE=/root/.cache/huggingface/datasets
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/transformers
      - HUGGINGFACE_HUB_CACHE=/root/.cache/huggingface/hub
      
      # API Keys (from .env file)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://192.168.80.200:11434}
    
    env_file:
      - .env
    
    volumes:
      # Notebooks - User projects and notebooks (DATA - persisted in dockerVolumes)
      - D:/dockerVolumes/conda/notebooks:/workspace
      
      # Conda environments - Virtual environments (LOCAL - persisted, gitignored)
      - ./envs:/opt/conda/envs
      
      # Conda packages cache - Downloaded packages cache (LOCAL - persisted, gitignored)
      - ./pkgs:/opt/conda/pkgs

      # HuggingFace models cache - Transformers models (DATA - persisted in dockerVolumes)
      - D:/dockerVolumes/hf_cache:/root/.cache/huggingface
      
      # Conda configuration (INFRASTRUCTURE - versioned in Git)
      - ./config/.condarc:/root/.condarc
    
    working_dir: /workspace
    
    # Reiniciar automaticamente si falla
    restart: unless-stopped
    
    # Recursos + GPU
    deploy:
      resources:
        limits:
          cpus: "6"
          memory: 12G
        reservations:
          cpus: "3"
          memory: 6G
          devices:
            - driver: nvidia
              count: all  # Usa todas las GPUs disponibles (RTX 2080 Ti)
              capabilities: [gpu, compute, utility]
    
    # Healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/lab"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

networks:
  default:
    name: conda_network
